\section{Technical details}
How many hardware resources are there? The version playing against Mr Lee uses 1920 standard processor chips and 280 special ones developed originally to produce graphics for video games. At least part of the reason AlphaGo is ahead of the competition is that it runs on more potent hardware.

The basic idea is, use the old methods, but combine them. 

The search space is too large, so it is not possible to traverse the whole space to get the final result. 

Two networks, 

The policy network, this part is to imitate the bahaviors of human. Given the current state of the board, the network will return some promising-looking moves. And these information is fed into the value network. The value network will evaluate these moves. Basically based on the previous games.

Deep learning is based on two things: plenty of processing grunt and large amount of data. DeepMind trained its machine on a sample of 30m Go positions culled from online servers where amateurs and professionals gather to play. And by having AlphaGo play against another, slightly tweaked version of itself, more training data can be generated quickly.

Why is it meaningful? The techniques employed in AlphaGo can be used to teach computers to recognize faces, translate between languages, show relevant advertisements to internet users or hunt for subatomic particles in data from atom-smashers. Deep learning is thus a booming business. It powers the increasingly effective image- and voice-recognition abilities of computers, and firms such as Google, Facebook and Baidu are throwing money at it.

\subsection{Nature paper}
Two parts, policy network and value network. It's trained using supervised learning and reinforcement learning.

All game of perfect inforamtion has a optimal value function, which determines the outcome of the game. Two factors deterimines the the complexity of the game, braching factor and depth. To improve the performance, the effective search space can be reduced by two general principles. First, the depth of the search may be reduced by position evaluation: truncating the search space at state s and replacing the subtree below s by an approapriate value function that predicts the outcome from state s;Second, the breadth of the search may be reduced by sampling actions from a policy p(a|s) that is a probability distribution over possible moves a in position s.

Monte Carlo tree search (MCTS), 