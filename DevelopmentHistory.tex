\section{background}
% The following information is from this link http://www.nature.com/news/computer-science-the-learning-machines-1.14481
Computer science: The learning machines
\subsection{1943}
Warren McCulloch and Walter Pitts created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.
\subsection{1950s}
Back in the 1950s, when computers were new, the first generation of AI researchers eagerly predicted that fully fledged AI was right around the corner. But that optimism faded as researchers began to grasp the vast complexity of real-world knowledge — particularly when it came to perceptual problems such as what makes a face a human face, rather than a mask or a monkey face. Hundreds of researchers and graduate students spent decades hand-coding rules about all the different features that computers needed to identify objects.
\subsection{1980s}
In the 1980s, one better way seemed to be deep learning in neural networks. These systems promised to learn their own rules from scratch, and offered the pleasing symmetry of using brain-inspired mechanics to achieve brain-like function. The strategy called for simulated neurons to be organized into several layers. Give such a system a picture and the first layer of learning will simply notice all the dark and light pixels. The next layer might realize that some of these pixels form edges; the next might distinguish between horizontal and vertical lines. Eventually, a layer might recognize eyes, and might realize that two eyes are usually present in a human face.
\subsubsection{Not so successful}
The first deep-learning programs did not perform any better than simpler systems. And they were tricky to work with. The networks needed a rich stream of examples to learn from - like a baby gathering information about the world. There was not much digital information available, and it took too long for computers to crunch through what did exist. Applications were rare. One of the few was a technique - developed by LeCun - that is now used by banks to read handwritten cheques.
\subsection{Andrew Ng}
Meanwhile, Ng had convinced Google to let him use its data and computers on what became Google Brain. The project's ability to spot cats was a compelling (but not, on its own, commercially viable) demonstration of unsupervised learning — the most difficult learning task, because the input comes without any explanatory information such as names, titles or categories. But Ng soon became troubled that few researchers outside Google had the tools to work on deep learning.

So back at Stanford, Ng started developing bigger, cheaper-learning networks using graphics processing units (GPUs) - the super-fast chips developed for home-computer gaming. 

\subsection{2013, 3 years ago}
Three years ago, researchers at the secretive Google X lab in Mountain View, Califonia, extracted some 10 million still images from YouTube videos and fed them into Google Brain — a network of 1,000 computers programmed to soak up the world much as a human toddler does. After three days looking for recurring patterns, Google Brain decided, all on its own, that there were certain repeating categories it could identify: human faces, human bodies and … cats.